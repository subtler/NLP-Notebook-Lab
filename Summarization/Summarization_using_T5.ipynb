{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dddaf58-0c44-439b-ac6a-56bf36b2ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfe391-14af-42b4-b268-9d99c64cb231",
   "metadata": {},
   "source": [
    "# Use pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fab2be4-409f-4967-a8cf-ae1011a6c131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4559a30d94eb492482e1ca9bb856337d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff3f01f4f6a4cde9b33737a5ea93875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eff527b592b4ea78272029fe5aec800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9112d4382d9b49c09812554d292f292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6b6b08431e46ac838504af7dac5c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a97492f4d834426a1f0860d310c2ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization',model='t5-small')# Faster, smaller\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Better quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3daba6-a394-4eb7-9c6d-4e068eda0218",
   "metadata": {},
   "source": [
    "# Summarize Your Own Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2f7cc0-d01e-402b-a756-a4440fd88eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: the real breakthrough of LLMs lies in their transformer architecture and self-attention mechanisms that enable the models to weigh the importance of different parts of the input data . deep learning algorithms learn to predict not just the next word that should occur in\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "How do large language models work?\n",
    "LLMs run on neural networks—computational models with nodes clustered together like the neurons in a biological brain. This structure enables fast, parallel processing of signals and improves pattern recognition and deep learning.\n",
    "\n",
    "But the real breakthrough of LLMs lies in their transformer architecture and the self-attention mechanisms that enable the models to weigh the importance of different parts of the input data. LLMs can then predict a sequence of what should come next, rather like an auto-complete function. LLMs sift through billions or even trillions of data set parameters in their semantic analysis as they work to develop an understanding of the meaning of words in the specific context they’re being used.\n",
    "\n",
    "Over time, deep learning algorithms learn to predict not just the next word that should occur in the sentence, but beyond to the next paragraph and sometimes even the next section. This process is how an LLM bridges the gap between the underlying structure of data and the core business concepts it needs to be able to generate relevant content.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "summary = summarizer(text, max_length = 50, min_length = 20, do_sample=False)\n",
    "print('summary:', summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b6a88-e55b-42c0-a19e-9fec00e52019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
