{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b751ccd3-f992-4965-95ce-e32a57c391db",
   "metadata": {},
   "source": [
    "# BLOCK 1: Load & Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ee69a1-d95a-45e0-bcff-345fe233ff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 10269\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1283\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1284\n",
      "    })\n",
      "})\n",
      "{'id': '2635.json', 'label': 0, 'statement': 'Says the Annies List political group supports third-trimester abortions on demand.', 'subject': 'abortion', 'speaker': 'dwayne-bohac', 'job_title': 'State representative', 'state_info': 'Texas', 'party_affiliation': 'republican', 'barely_true_counts': 0.0, 'false_counts': 1.0, 'half_true_counts': 0.0, 'mostly_true_counts': 0.0, 'pants_on_fire_counts': 0.0, 'context': 'a mailer'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the LIAR dataset from HuggingFace\n",
    "dataset = load_dataset(\"liar\")\n",
    "\n",
    "# Check available splits\n",
    "print(dataset)\n",
    "\n",
    "# View a few examples\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053df653-f8b0-4983-bc47-ab3e8734db00",
   "metadata": {},
   "source": [
    "# BLOCK 2: Preprocess — Label Mapping & Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0210c2-e9e3-4c55-9948-69f84bd89d45",
   "metadata": {},
   "source": [
    "## Step 1: Confirm Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96651a35-b73f-43f7-915e-50e6ce87d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"pants-fire\",\n",
    "    1: \"false\",\n",
    "    2: \"barely-true\",\n",
    "    3: \"half-true\",\n",
    "    4: \"mostly-true\",\n",
    "    5: \"true\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b506400-a777-44c5-b511-47a5b85fb4e6",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize the “statement” column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b4e9c9-4140-40db-9635-56074d166c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"statement\"],\n",
    "        padding=\"max_length\",  # or \"longest\" during testing\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2535d-d9a6-4b08-b5d8-3a058b031ce8",
   "metadata": {},
   "source": [
    "## Step 3: Apply Tokenizer Across Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b4a2be-96a5-4dec-b3ba-ecea73366629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all splits\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca85d81-73ff-4807-959d-9964af157bfd",
   "metadata": {},
   "source": [
    "This adds:\n",
    "\t•\tinput_ids\n",
    "\t•\tattention_mask\n",
    "\t•\tKeeps label as is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e2f2d-9238-48ab-9a33-ed0b5adf6fea",
   "metadata": {},
   "source": [
    "# BLOCK 3: Prepare the Dataset for HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f51b1-ab1a-4218-9413-10e812d27cbb",
   "metadata": {},
   "source": [
    "## Step 1: Keep Only Required Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410f3ef-13bb-4d0f-8368-922c164ecf95",
   "metadata": {},
   "source": [
    "The model only needs:\n",
    "\t•\tinput_ids\n",
    "\t•\tattention_mask\n",
    "\t•\tlabel, So we’ll remove all other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831e774c-ffb3-448e-b3d9-eb735fa515f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [col for col in tokenized_datasets[\"train\"].column_names if col not in [\"input_ids\", \"attention_mask\", \"label\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8dfaf6-23c3-4dda-b85d-c90171505e87",
   "metadata": {},
   "source": [
    "## Step 2: Set Dataset Format for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06eff95-3043-4c39-a4ba-56eee1e3bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424f107-2ea2-400c-9309-3be9610a3390",
   "metadata": {},
   "source": [
    "## Step 3: Split into Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe865e95-f209-46e3-bdde-0bf226b6886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b86d9f-89c4-4deb-95bf-0278f82a897a",
   "metadata": {},
   "source": [
    "This clean split ensures:\n",
    "\t•\ttrain_dataset → for model learning\n",
    "\t•\teval_dataset → for validation during training\n",
    "\t•\ttest_dataset → for final evaluation after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17fdb0-4b56-495f-a09a-0774e0bec135",
   "metadata": {},
   "source": [
    "# BLOCK 4: Load DistilBERT Model for 6-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61d298-55d4-4471-9265-698b29a9cb46",
   "metadata": {},
   "source": [
    "## Step 1: Define Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2061c34-9e9f-4e8a-ab8b-055932368a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"pants-fire\",\n",
    "    1: \"false\",\n",
    "    2: \"barely-true\",\n",
    "    3: \"half-true\",\n",
    "    4: \"mostly-true\",\n",
    "    5: \"true\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96507bab-5dbb-4ade-9b06-8eea39b05292",
   "metadata": {},
   "source": [
    "## Step 2: Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3acd27d-e569-4d4a-8a57-17a8229920d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=6,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf8947-a712-45fc-bcd7-528e60c10d6a",
   "metadata": {},
   "source": [
    "# BLOCK 5: Define TrainingArguments & Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946f855-eb52-4081-bc91-52a3c7138844",
   "metadata": {},
   "source": [
    "## Step 1: Define TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9beb8b6b-c975-4fe3-82dc-7574544655bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Updated parameter name\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc463bd-cb0d-4fd9-a9cd-f1b9618c06a7",
   "metadata": {},
   "source": [
    "## Step 2: Define a Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06dfcb79-5e14-4081-9c07-a0156b88e86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4947f1d700342b3a80504079fdefb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab71a6-8a30-4411-b1b4-15a820baa104",
   "metadata": {},
   "source": [
    "## Step 3: Create the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2ba72a-7534-433f-bbec-6c6a5742f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/375whh5x1pldcb3r78dqhr2h0000gn/T/ipykernel_36227/1757859548.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab09ff-89bc-4442-9cf4-1da492e6562c",
   "metadata": {},
   "source": [
    "## Step: BLOCK 6 — Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975db11-2966-4ccd-bb41-3533eb409e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a959f3-5fde-47a2-9143-9edf22b502e7",
   "metadata": {},
   "source": [
    "This will:\n",
    "\t•\tStart fine-tuning DistilBERT on your LIAR dataset\n",
    "\t•\tShow training + validation loss\n",
    "\t•\tSave model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5089e74-bf7d-4dea-8ae4-529de5347ca4",
   "metadata": {},
   "source": [
    "## Step 2: Save the Fine-Tuned Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b45f14-53da-4037-a7c5-0af14fd4a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"../models/fine_tuned_distilbert_liar\"\n",
    "\n",
    "# Save model & tokenizer\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive(model_save_path, 'zip', model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a68254-96da-431a-ae2c-a12d746555f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download\n",
    "files.download(f\"{model_save_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41451f6f-6d21-4fcb-ae2b-37e7253f1b20",
   "metadata": {},
   "source": [
    "# BLOCK 7: Evaluate Fine-Tuned Model on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93266f0-dfb5-4e37-b673-09d643790a3d",
   "metadata": {},
   "source": [
    "## Step 1: Evaluate Using trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dcb09-0d33-4915-87f8-304f60001ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"📊 Test Set Evaluation Metrics:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156adc9c-d909-4b53-8ae8-2d8a09f09ecd",
   "metadata": {},
   "source": [
    "This gives you:\n",
    "\t•\teval_loss\n",
    "\t•\teval_accuracy\n",
    "\t•\t(More, if additional metrics are added like F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e8279-5f73-4d2c-8395-5f6781fe8cd2",
   "metadata": {},
   "source": [
    "## Step 2: Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b767a-e176-4608-adbf-a5fe98b9f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=list(id2label.values())))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=id2label.values(), yticklabels=id2label.values(), cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
